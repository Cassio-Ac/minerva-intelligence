"""
LLM Service
Processa mensagens do usu√°rio com LLM (Claude via Databricks)
"""

from typing import Optional, Dict, Any, List
import logging
import json
import os

logger = logging.getLogger(__name__)


class LLMService:
    """Service para processamento com LLM"""

    def __init__(self, use_real_llm: bool = True):
        """
        Inicializa o service

        Args:
            use_real_llm: Se True, tenta usar Databricks. Se False ou falhar, usa mock.
        """
        self.llm_client = None
        self.llm_available = False

        if use_real_llm:
            try:
                self._initialize_databricks()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not initialize Databricks LLM: {e}")
                logger.info("üìù Falling back to mock responses")

    def _initialize_databricks(self):
        """Inicializa cliente Databricks"""
        from app.core.config import settings

        databricks_url = settings.DATABRICKS_URL or settings.DATABRICKS_HOST
        databricks_token = settings.DATABRICKS_TOKEN
        model_name = settings.DATABRICKS_MODEL or settings.LLM_MODEL

        if not databricks_url or not databricks_token:
            raise ValueError("DATABRICKS_URL and DATABRICKS_TOKEN must be set in .env")

        from app.services.databricks_client import DatabricksChatClient

        self.llm_client = DatabricksChatClient(
            databricks_url=databricks_url,
            databricks_token=databricks_token,
            model_name=model_name,
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS
        )

        self.llm_available = True
        logger.info(f"‚úÖ Databricks LLM initialized: {model_name}")


    async def process_message(
        self,
        message: str,
        index: str,
        server_id: Optional[str] = None,
        time_range: Optional[Dict] = None,
        context: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Processa mensagem do usu√°rio e retorna widget spec

        Args:
            message: Mensagem do usu√°rio
            index: √çndice Elasticsearch
            server_id: ID do servidor ES (opcional)
            time_range: Per√≠odo temporal para filtro (opcional)
            context: Hist√≥rico de mensagens (opcional)

        Returns:
            {
                "explanation": str,
                "visualization_type": str,
                "query": dict,
                "needs_clarification": bool,
                "widget": dict
            }
        """
        logger.info(
            f"Processing message: {message} (index: {index}, "
            f"server: {server_id or 'default'}, time_range: {time_range}, "
            f"llm_available: {self.llm_available})"
        )

        # SEMPRE usa LLM real quando dispon√≠vel (para todas as mensagens)
        if self.llm_available and self.llm_client:
            try:
                logger.info("üéØ Using Databricks LLM for all message processing")
                return await self._process_with_real_llm(message, index, server_id, time_range, context)
            except Exception as e:
                logger.error(f"‚ùå Error with real LLM, falling back to mock: {e}")
                # Continua para processamento mock em caso de erro

        # Fallback: processamento mock (s√≥ se LLM n√£o dispon√≠vel ou falhar)
        logger.warning("üìù Using mock processing (LLM not available or failed)")
        message_lower = message.lower()

        # Primeiro, detectar se √© uma mensagem conversacional (n√£o precisa de widget)
        if self._is_conversational(message_lower):
            response = self._generate_conversational_response(message_lower)
            return {
                "explanation": response,
                "visualization_type": None,
                "query": None,
                "needs_clarification": False,
                "widget": None
            }

        # Detectar se √© uma pergunta geral (n√£o sobre visualiza√ß√£o)
        if self._is_general_question(message_lower):
            response = self._answer_general_question(message_lower)
            return {
                "explanation": response,
                "visualization_type": None,
                "query": None,
                "needs_clarification": False,
                "widget": None
            }

        # Detectar se realmente quer uma visualiza√ß√£o
        if not self._wants_visualization(message_lower):
            return {
                "explanation": "Posso ajudar com visualiza√ß√µes de dados! Me diga o que voc√™ gostaria de ver:\n\n‚Ä¢ Gr√°fico de pizza para distribui√ß√µes\n‚Ä¢ Gr√°fico de barras para compara√ß√µes\n‚Ä¢ Gr√°fico de linhas para tend√™ncias\n‚Ä¢ M√©trica para valores √∫nicos\n\nExemplo: 'mostre um gr√°fico de pizza' ou 'quero ver o total'",
                "visualization_type": None,
                "query": None,
                "needs_clarification": True,
                "widget": None
            }

        # Detectar tipo de visualiza√ß√£o
        viz_type = self._detect_visualization_type(message_lower)

        # Gerar query mock
        query = self._generate_mock_query(message_lower, viz_type)

        # Gerar dados mock
        widget_data = self._generate_mock_data(viz_type)

        result = {
            "explanation": f"Criei uma visualiza√ß√£o do tipo {viz_type} para mostrar {message}",
            "visualization_type": viz_type,
            "query": query,
            "needs_clarification": False,
            "widget": {
                "title": self._generate_title(message),
                "type": viz_type,
                "data": {
                    "query": query,
                    "results": {},
                    "config": widget_data
                }
            }
        }

        logger.info(f"‚úÖ Generated {viz_type} visualization")
        return result

    async def _process_with_real_llm(
        self,
        message: str,
        index: str,
        server_id: Optional[str] = None,
        time_range: Optional[Dict] = None,
        context: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Processa mensagem usando Databricks LLM real

        Args:
            message: Mensagem do usu√°rio
            index: √çndice Elasticsearch
            server_id: ID do servidor ES (opcional)
            time_range: Per√≠odo temporal para filtro (opcional)
            context: Hist√≥rico de mensagens

        Returns:
            Dicion√°rio com resposta processada
        """
        from langchain_core.messages import HumanMessage, SystemMessage

        # Gerar base de conhecimento completa do √≠ndice
        from app.services.index_mapping_service import get_mapping_service
        mapping_service = get_mapping_service()
        knowledge_base = await mapping_service.generate_knowledge_base(index)

        # Build system prompt
        from datetime import datetime
        now = datetime.now()
        data_hoje = now.strftime("%d/%m/%Y")
        dia_semana = ["segunda", "ter√ßa", "quarta", "quinta", "sexta", "s√°bado", "domingo"][now.weekday()]
        hora_atual = now.strftime("%H:%M")

        # Formatar time_range se fornecido
        time_range_info = ""
        if time_range:
            time_range_dict = time_range if isinstance(time_range, dict) else time_range.dict()
            time_range_info = f"""
**PER√çODO TEMPORAL SELECIONADO:**
- Tipo: {time_range_dict.get('type', 'preset')}
- Per√≠odo: {time_range_dict.get('label', 'N√£o definido')}
- De: {time_range_dict.get('from') or time_range_dict.get('from_', 'now-30d')}
- At√©: {time_range_dict.get('to', 'now')}

**‚ö†Ô∏è CR√çTICO - FILTRO TEMPORAL OBRIGAT√ìRIO:**
- TODA query DEVE incluir filtro temporal em um campo DATE
- Escolha o campo DATE mais apropriado da base de conhecimento acima
- Use EXATAMENTE estes valores:
  - gte: {time_range_dict.get('from') or time_range_dict.get('from_', 'now-30d')}
  - lte: {time_range_dict.get('to', 'now')}

**Como escolher o campo de data:**
- Procure na base de conhecimento por campos tipo DATE
- Use o campo mais relevante (ex: @timestamp, data_criacao, scan_date, etc)
- Se o √≠ndice n√£o tiver campo de data, N√ÉO adicione o filtro temporal

**Estrutura OBRIGAT√ìRIA da query (se houver campo DATE):**
{{
  "size": 0,
  "query": {{
    "bool": {{
      "filter": [
        {{
          "range": {{
            "CAMPO_DATE_AQUI": {{
              "gte": "{time_range_dict.get('from') or time_range_dict.get('from_', 'now-30d')}",
              "lte": "{time_range_dict.get('to', 'now')}"
            }}
          }}
        }}
      ]
    }}
  }},
  "aggs": {{ ... suas agrega√ß√µes aqui ... }}
}}

IMPORTANTE: Substitua "CAMPO_DATE_AQUI" pelo campo DATE real do √≠ndice!
"""
        else:
            time_range_info = """
**PER√çODO TEMPORAL:**
- Nenhum per√≠odo espec√≠fico selecionado
- Use "now-30d" a "now" como padr√£o se precisar filtrar por data
"""

        system_prompt = f"""Voc√™ √© Claude (Sonnet 4.5), um assistente conversacional especializado em an√°lise de dados do Elasticsearch.

**CONTEXTO ATUAL:**
- Data de hoje: {dia_semana}-feira, {data_hoje}
- Hora atual: {hora_atual}
- √çndice Elasticsearch em uso: **{index}**
{time_range_info}
**BASE DE CONHECIMENTO DO √çNDICE:**
{knowledge_base}

**SUA PERSONALIDADE:**
- Seja conversacional, amig√°vel e natural
- Responda perguntas gerais (data, hora, sauda√ß√µes, d√∫vidas)
- Use emojis ocasionalmente para dar personalidade
- Seja proativo e ofere√ßa ajuda

**SUAS CAPACIDADES:**
1. **Conversar naturalmente** sobre qualquer assunto
2. **Responder perguntas** sobre data, hora, seus recursos
3. **Criar visualiza√ß√µes de dados** do Elasticsearch quando solicitado
4. **Explicar** como funciona a an√°lise de dados

**IMPORTANTE:**
- Para sauda√ß√µes, perguntas gerais ou conversas, responda naturalmente SEM criar visualiza√ß√µes
- S√≥ crie visualiza√ß√µes quando o usu√°rio explicitamente pedir para ver/mostrar/criar gr√°ficos ou dados

Para pedidos de visualiza√ß√£o, retorne JSON no formato:
{{
    "needs_visualization": true,
    "visualization_type": "pie" | "bar" | "line" | "metric",
    "title": "T√≠tulo Profissional do Widget",
    "query": {{ ... elasticsearch query ... }},
    "explanation": "Explica√ß√£o em portugu√™s do que foi criado"
}}

**REGRAS PARA T√çTULOS DOS WIDGETS:**
- Crie t√≠tulos PROFISSIONAIS, CONCISOS e DESCRITIVOS
- Use portugu√™s formal e claro
- Evite artigos no in√≠cio ("o", "a", "os", "as")
- Exemplos CORRETOS:
  * "Distribui√ß√£o por Categoria"
  * "Top 10 Dom√≠nios Mais Acessados"
  * "Evolu√ß√£o Temporal de Acessos"
  * "Total de Vulnerabilidades"
  * "Vulnerabilidades por Severidade"
- Exemplos INCORRETOS:
  * "mostre um gr√°fico de pizza" ‚ùå
  * "gr√°fico com categorias" ‚ùå
  * "total" ‚ùå (muito gen√©rico)

Para perguntas gerais, retorne JSON:
{{
    "needs_visualization": false,
    "explanation": "Sua resposta aqui"
}}

**Tipos de visualiza√ß√£o dispon√≠veis:**
- **pie**: Gr√°ficos de pizza para distribui√ß√µes percentuais
- **bar**: Gr√°ficos de barras para compara√ß√µes entre categorias
- **line**: Gr√°ficos de linhas para tend√™ncias temporais
- **area**: Gr√°ficos de √°rea para volumes ao longo do tempo
- **metric**: M√©trica √∫nica (contagem total, soma, etc)
- **table**: Tabela para visualizar dados detalhados em formato tabular
- **scatter**: Gr√°fico de dispers√£o para correla√ß√µes

**Exemplos de queries Elasticsearch COM FILTRO TEMPORAL:**
(Use o campo DATE apropriado do √≠ndice, n√£o necessariamente @timestamp)

1. Total de documentos (metric) - COM FILTRO TEMPORAL:
{{
  "query": {{
    "bool": {{
      "filter": [
        {{"range": {{"<campo_date_do_indice>": {{"gte": "now-30d", "lte": "now"}}}}}}
      ]
    }}
  }}
}}

2. Distribui√ß√£o por campo (pie/bar) - COM FILTRO TEMPORAL:
{{
  "size": 0,
  "query": {{
    "bool": {{
      "filter": [
        {{"range": {{"<campo_date_do_indice>": {{"gte": "now-7d", "lte": "now"}}}}}}
      ]
    }}
  }},
  "aggs": {{"result": {{"terms": {{"field": "campo.keyword", "size": 10}}}}}}
}}

3. Histograma temporal (line/area) - COM FILTRO TEMPORAL:
{{
  "size": 0,
  "query": {{
    "bool": {{
      "filter": [
        {{"range": {{"<campo_date_do_indice>": {{"gte": "now-30d", "lte": "now"}}}}}}
      ]
    }}
  }},
  "aggs": {{"result": {{"date_histogram": {{"field": "<campo_date_do_indice>", "calendar_interval": "day"}}}}}}
}}

4. Tabela com documentos (table) - COM FILTRO TEMPORAL:
{{
  "size": 10,
  "query": {{
    "bool": {{
      "filter": [
        {{"range": {{"<campo_date_do_indice>": {{"gte": "now-24h", "lte": "now"}}}}}}
      ]
    }}
  }}
}}

**IMPORTANTE - REGRAS PARA CRIA√á√ÉO DE QUERIES:**
- ‚ö†Ô∏è **FILTRO TEMPORAL √â OBRIGAT√ìRIO**: Toda query DEVE ter filtro no campo DATE apropriado usando os valores fornecidos
- **USE APENAS OS CAMPOS** listados na base de conhecimento acima
- **RESPEITE A AGREGABILIDADE**: Use apenas campos marcados com ‚úì AGREG√ÅVEL para agrega√ß√µes
- **CAMPOS TEXT**: Para campos text, siga as instru√ß√µes üí° (geralmente usar .keyword)
- **CAMPOS KEYWORD**: Use diretamente para agrega√ß√µes (j√° s√£o keyword)
- **CAMPOS DATE**: Use para date_histogram com calendar_interval adequado
- **CAMPOS NUM√âRICOS**: Use para m√©tricas (sum, avg, min, max, stats)
- SEMPRE use "result" como nome da agrega√ß√£o principal
- Retorne APENAS o JSON, sem markdown
- Seja conversacional e amig√°vel"""

        # Adicionar contexto se houver
        if context and len(context) > 0:
            context_str = "\n\n**Hist√≥rico recente:**\n"
            for msg in context[-3:]:  # √öltimas 3 mensagens
                # Suporta tanto dict quanto objeto Pydantic
                if isinstance(msg, dict):
                    role = "Usu√°rio" if msg.get("role") == "user" else "Assistente"
                    content = msg.get("content", "")
                else:
                    # Objeto Pydantic ChatMessage
                    role = "Usu√°rio" if msg.role == "user" else "Assistente"
                    content = msg.content
                context_str += f"- {role}: {content}\n"
            system_prompt += context_str

        # Chamar LLM
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=message)
        ]

        logger.info("ü§ñ Calling Databricks LLM...")
        response = self.llm_client.invoke(messages)
        content = response.content.strip()

        logger.info(f"üì• LLM Response ({len(content)} chars)")

        # Parse JSON response
        result = self._parse_llm_json_response(content)

        # Converter para formato esperado
        if result.get("needs_visualization"):
            viz_type = result.get("visualization_type", "pie")
            query = result.get("query", {})

            # Executar query no Elasticsearch para obter dados reais
            widget_data = {}
            try:
                from app.services.elasticsearch_service import get_es_service
                es_service = get_es_service()

                logger.info(f"üîç Executing ES query to get real data...")
                es_results = await es_service.execute_query(index, query)

                # Formatar dados para o widget
                widget_data = {
                    "data": es_results.get("data", [])
                }

                logger.info(f"‚úÖ Got {len(es_results.get('data', []))} data points from ES")

            except Exception as e:
                logger.error(f"‚ùå Error executing ES query: {e}")
                # Em caso de erro, usar dados vazios
                widget_data = {"data": []}

            # Usar t√≠tulo da LLM se dispon√≠vel, sen√£o gerar
            widget_title = result.get("title") or self._generate_title(message)

            return {
                "explanation": result.get("explanation", "Visualiza√ß√£o criada"),
                "visualization_type": viz_type,
                "query": query,
                "needs_clarification": False,
                "widget": {
                    "title": widget_title,
                    "type": viz_type,
                    "data": {
                        "query": query,
                        "results": es_results if 'es_results' in locals() else {},
                        "config": widget_data
                    }
                }
            }
        else:
            # Resposta conversacional
            return {
                "explanation": result.get("explanation", ""),
                "visualization_type": None,
                "query": None,
                "needs_clarification": False,
                "widget": None
            }

    def _parse_llm_json_response(self, content: str) -> Dict[str, Any]:
        """
        Parse da resposta JSON do LLM

        Args:
            content: Resposta do LLM

        Returns:
            Dicion√°rio parseado
        """
        import re

        # Remove markdown code blocks se houver
        if "```json" in content:
            content = re.sub(r'```json\s*', '', content)
            content = re.sub(r'```\s*$', '', content)
        elif "```" in content:
            content = re.sub(r'```\s*', '', content)

        content = content.strip()

        # Procura por JSON
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå JSON parse error: {e}")
                logger.error(f"Content: {json_str[:200]}...")

        # Se n√£o conseguiu parsear, retorna resposta conversacional
        return {
            "needs_visualization": False,
            "explanation": content
        }

    async def _get_index_fields(self, index: str) -> str:
        """
        Busca os campos dispon√≠veis no √≠ndice Elasticsearch

        Args:
            index: Nome do √≠ndice

        Returns:
            String formatada com os campos
        """
        try:
            from app.services.elasticsearch_service import get_es_service
            es_service = get_es_service()

            mapping = await es_service.get_index_mapping(index)

            # Extrair campos do mapping
            fields = []
            if index in mapping:
                properties = mapping[index].get("mappings", {}).get("properties", {})

                for field_name, field_props in properties.items():
                    field_type = field_props.get("type", "unknown")
                    fields.append(f"- **{field_name}** ({field_type})")

            if fields:
                return "\n".join(fields)
            else:
                return "Nenhum campo encontrado (√≠ndice vazio ou sem mapping)"

        except Exception as e:
            logger.warning(f"Could not fetch index mapping: {e}")
            return "Campos n√£o dispon√≠veis (use nomes comuns como 'source', 'type', etc)"

    def _is_conversational(self, message: str) -> bool:
        """Detecta se √© uma mensagem conversacional (sauda√ß√£o, agradecimento, etc)"""
        conversational_patterns = [
            # Sauda√ß√µes (palavra completa ou no in√≠cio)
            (r'\boi\b', r'^oi\s'),
            (r'\bol√°\b', r'^ol√°\s', r'\bola\b', r'^ola\s'),
            (r'\bhey\b', r'\bhi\b', r'\bhello\b'),
            # Agradecimentos
            (r'\bobrigad[oa]\b', r'\bvaleu\b', r'\bthanks\b', r'\bthank you\b'),
            # Despedidas
            (r'\btchau\b', r'\bat√© logo\b', r'\bat√© mais\b', r'\bbye\b', r'\bgoodbye\b'),
            # Perguntas gerais
            (r'\btudo bem\b', r'\bcomo vai\b', r'\be ai\b', r'\be a√≠\b'),
            # Confirma√ß√µes simples (apenas palavra sozinha)
            (r'^\s*ok\s*$', r'^\s*beleza\s*$', r'^\s*legal\s*$', r'^\s*show\s*$', r'^\s*boa\s*$')
        ]

        import re

        # Verificar se a mensagem √© muito curta (at√© 3 palavras)
        if len(message.split()) <= 3:
            for patterns in conversational_patterns:
                for pattern in patterns if isinstance(patterns, tuple) else (patterns,):
                    if re.search(pattern, message, re.IGNORECASE):
                        return True

        return False

    def _generate_conversational_response(self, message: str) -> str:
        """Gera resposta conversacional apropriada"""
        # Sauda√ß√µes
        if any(word in message for word in ["oi", "ol√°", "ola", "hey", "hi", "hello"]):
            return "Ol√°! Como posso ajudar voc√™ a visualizar seus dados? Voc√™ pode me pedir para criar gr√°ficos de pizza, barras, linhas ou mostrar m√©tricas. üòä"

        # Agradecimentos
        if any(word in message for word in ["obrigado", "obrigada", "valeu", "thanks"]):
            return "De nada! Estou aqui para ajudar com suas visualiza√ß√µes. Precisa de mais alguma coisa?"

        # Despedidas
        if any(word in message for word in ["tchau", "at√© logo", "at√© mais", "bye"]):
            return "At√© logo! Volte sempre que precisar criar visualiza√ß√µes! üëã"

        # Perguntas sobre estado
        if any(word in message for word in ["tudo bem", "como vai"]):
            return "Tudo √≥timo! Pronto para criar visualiza√ß√µes incr√≠veis para voc√™. O que gostaria de ver?"

        # Confirma√ß√µes
        if any(word in message for word in ["ok", "beleza", "legal", "show", "boa"]):
            return "√ìtimo! Se precisar de mais alguma visualiza√ß√£o, √© s√≥ pedir!"

        # Default
        return "Estou aqui para ajudar! Me diga o que voc√™ gostaria de visualizar."

    def _is_general_question(self, message: str) -> bool:
        """Detecta se √© uma pergunta geral (n√£o sobre visualiza√ß√£o)"""
        import re

        general_patterns = [
            # Perguntas sobre data/hora
            r'\b(que|qual).*\b(dia|data)\b',
            r'\b(que|qual).*\bhora[s]?\b',
            r'\bhora[s]?\s+(s√£o|sao|√©|e)\b',
            r'\bhoje\b', r'\bhj\b', r'\bagora\b',
            # Perguntas sobre o assistente
            r'\b(quem|o que|que).*\b(voc√™|vc|voce)\b',
            r'\bvoc√™ (√©|e)\b', r'\bvc (√©|e)\b',
            # Pedidos de ajuda
            r'\b(ajuda|help|como.*funciona|o que.*fazer)\b',
            r'\b(pode|consegue|sabe).*fazer\b',
            # Perguntas sobre capacidades
            r'\bque.*posso\b', r'\bo que.*posso\b',
        ]

        for pattern in general_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return True

        return False

    def _answer_general_question(self, message: str) -> str:
        """Responde perguntas gerais"""
        from datetime import datetime
        import re

        # Perguntas sobre capacidades (verificar primeiro, antes de "o que")
        if re.search(r'\b(o que|que).*\b(pode[s]?|consegue[s]?|sabe[s]?).*\bfazer\b', message, re.IGNORECASE):
            return """Posso fazer v√°rias coisas! üöÄ

‚úÖ Criar visualiza√ß√µes interativas (pizza, barras, linhas, m√©tricas)
‚úÖ Gerar queries Elasticsearch automaticamente
‚úÖ Responder perguntas sobre data, hora
‚úÖ Ajudar com d√∫vidas sobre o dashboard

Quer criar alguma visualiza√ß√£o?"""

        # Perguntas sobre data/hora
        if re.search(r'\b(que|qual).*\b(dia|data)\b|\bhoje\b|\bhj\b', message, re.IGNORECASE):
            now = datetime.now()
            data_formatada = now.strftime("%d/%m/%Y")
            dia_semana = ["segunda", "ter√ßa", "quarta", "quinta", "sexta", "s√°bado", "domingo"][now.weekday()]
            return f"Hoje √© {dia_semana}-feira, {data_formatada}. üìÖ\n\nPosso criar alguma visualiza√ß√£o para voc√™?"

        if re.search(r'\b(que|qual).*\bhora[s]?\b|\bhora[s]?\s+(s√£o|sao|√©|e)\b|\bagora\b', message, re.IGNORECASE):
            now = datetime.now()
            hora_formatada = now.strftime("%H:%M")
            return f"Agora s√£o {hora_formatada}. ‚è∞\n\nQuer visualizar algum dado?"

        # Pedidos de ajuda
        if re.search(r'\bajuda\b|\bhelp\b|\bcomo.*funciona\b', message, re.IGNORECASE):
            return """Posso ajudar voc√™ a criar visualiza√ß√µes! Aqui est√£o alguns exemplos:

üìä **Gr√°ficos dispon√≠veis:**
‚Ä¢ Pizza - "mostre a distribui√ß√£o por categoria"
‚Ä¢ Barras - "compare os valores por regi√£o"
‚Ä¢ Linhas - "mostre a tend√™ncia ao longo do tempo"
‚Ä¢ M√©trica - "qual o total de registros"

üí° **Dicas:**
‚Ä¢ Use linguagem natural
‚Ä¢ Seja espec√≠fico sobre o que quer ver
‚Ä¢ Posso tamb√©m responder perguntas gerais!

O que gostaria de visualizar?"""

        # Perguntas sobre o assistente
        if re.search(r'\b(quem|o que).*\b(voc√™|vc)\b|\bvoc√™ (√©|e)\b', message, re.IGNORECASE):
            return "Sou um assistente de visualiza√ß√£o de dados! ü§ñ\n\nPosso criar gr√°ficos e dashboards interativos a partir dos seus dados no Elasticsearch. Basta me dizer o que voc√™ quer ver!"

        # Default para outras perguntas
        return "Interessante pergunta! Minha especialidade √© criar visualiza√ß√µes de dados. üìä\n\nQue tal me dizer o que voc√™ gostaria de visualizar?"

    def _wants_visualization(self, message: str) -> bool:
        """Detecta se o usu√°rio realmente quer criar uma visualiza√ß√£o"""
        visualization_keywords = [
            # Verbos de a√ß√£o
            "mostre", "mostra", "exiba", "exibir", "crie", "criar", "gere", "gerar",
            "fa√ßa", "fazer", "quero", "preciso", "gostaria",
            # Tipos de gr√°fico
            "gr√°fico", "grafico", "chart", "visualiza√ß√£o", "visualizacao",
            "pizza", "pie", "barra", "bar", "linha", "line", "m√©trica", "metric",
            # An√°lises
            "total", "soma", "count", "m√©dia", "media", "distribui√ß√£o", "distribuicao",
            "comparar", "compara√ß√£o", "comparacao", "tend√™ncia", "tendencia",
            "evolu√ß√£o", "evolucao", "ranking", "top"
        ]

        return any(keyword in message for keyword in visualization_keywords)

    def _detect_visualization_type(self, message: str) -> str:
        """Detecta tipo de visualiza√ß√£o baseado na mensagem"""
        if any(word in message for word in ["pizza", "distribui√ß√£o", "porcentagem", "pie"]):
            return "pie"
        elif any(word in message for word in ["barra", "comparar", "ranking", "top", "bar"]):
            return "bar"
        elif any(word in message for word in ["linha", "tend√™ncia", "tempo", "evolu√ß√£o", "line"]):
            return "line"
        elif any(word in message for word in ["total", "soma", "count", "m√©trica", "metric"]):
            return "metric"
        else:
            # Default: pie
            return "pie"

    def _generate_mock_query(self, message: str, viz_type: str) -> Dict[str, Any]:
        """Gera query Elasticsearch mock"""
        if viz_type == "metric":
            return {
                "size": 0,
                "aggs": {
                    "total": {
                        "value_count": {"field": "_id"}
                    }
                }
            }
        else:
            return {
                "size": 0,
                "aggs": {
                    "data": {
                        "terms": {
                            "field": "category.keyword",
                            "size": 10
                        }
                    }
                }
            }

    def _generate_mock_data(self, viz_type: str) -> Dict[str, Any]:
        """Gera dados mock para visualiza√ß√£o"""
        if viz_type == "metric":
            return {
                "data": [
                    {"label": "Total", "value": 1234}
                ]
            }
        elif viz_type == "pie":
            return {
                "data": [
                    {"label": "Categoria A", "value": 35},
                    {"label": "Categoria B", "value": 25},
                    {"label": "Categoria C", "value": 20},
                    {"label": "Categoria D", "value": 15},
                    {"label": "Outros", "value": 5}
                ]
            }
        elif viz_type == "bar":
            return {
                "data": [
                    {"label": "Jan", "value": 120},
                    {"label": "Fev", "value": 150},
                    {"label": "Mar", "value": 180},
                    {"label": "Abr", "value": 160},
                    {"label": "Mai", "value": 200}
                ]
            }
        elif viz_type == "line":
            return {
                "data": [
                    {"label": "Seg", "value": 30},
                    {"label": "Ter", "value": 45},
                    {"label": "Qua", "value": 38},
                    {"label": "Qui", "value": 52},
                    {"label": "Sex", "value": 48},
                    {"label": "S√°b", "value": 35},
                    {"label": "Dom", "value": 25}
                ]
            }
        else:
            return {"data": []}

    def _generate_title(self, message: str) -> str:
        """Gera t√≠tulo baseado na mensagem"""
        # Capitalizar primeira letra de cada palavra
        return message.strip().capitalize()


# Singleton instance
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """Retorna inst√¢ncia do service"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
